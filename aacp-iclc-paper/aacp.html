<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>MosAIk: staging contemporary AI performance - reflections on connecting live coding, e-textile and movement…</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="css/iclc.css" />
</head>
<body>
<div id="header">
<h1 class="title">MosAIk: staging contemporary AI performance -
reflections on connecting live coding, e-textile and movement…</h1>
<ul id="authorlist">
<li>true</li>
<li>true</li>
<li>true</li>
<li>true</li>
<li>true</li>
</ul>
</div>

<h2 class="abstract">Abstract</h2>
<div id="abstract">
<p>This paper introduces our collective work “Patterns in between
intelligences”, a performance piece that builds an artistic practice
between live coding sounds and coding through dance, mediated and shaped
through e-textile sensors. This creates a networked system of which both
live coded processes and human bodies are part. The paper describes in
detail the implementations of technology used in the prototype
performance performed at No Bounds Festival in Sheffield UK, October
2022, as well as discussions and concerns the team had related to the
use of AI technology on stage. The paper concludes with a narrative
reflection on the Sheffield performance, and reflections on it.</p>
</div>

<h1 data-number="1" id="introduction"><span
class="header-section-number">1</span> Introduction</h1>
<p>The project began in September 2020 as a response to a funding call
by the ‘LINK masters’ initiative to produce an artistic work using
Artificial Intelligence. Our multidisciplinary team: choreographers and
performers, live coders and e-textile makers gathered together to
strategise how AI could inform an artistic performance work, both in
terms of the concepts from AI that could be explored through the work,
and how AI tools could be explored to realise the work.</p>
<p>In terms of AI concepts, we wanted to challenge mainstream ideas of
AI, such as the emulation of human neural pathways as the method of
creating new intelligences. Instead, we decided to take a collective
view of intelligence. In developing an approach to performance we
created a framework of our own ritual, to call into question the
spiritual and social dimension of AI, and to connect machine learning
with patterning techniques informed by ancient traditions. In terms of
tools, a wide range of technologies have been branded as AI. Accordingly
we have explored a range of techniques, with some focus on machine
learning, machine listening and neural audio synthesis.</p>
<p>By treating intelligence as collaborative, we undermine the dualist
thinking of body/mind as well as science/fiction, and open ourselves for
a more human-centric approach to AI, in terms of pattern generation,
pattern recognition and connectedness, but also resonance, textility,
perhaps even spirituality. Inspired by public ceremonies and formalistic
celebrations performed by humans since ancient times, the final
performance hosts a collective, healing ritual that searches for a new
spiritual code.</p>
<h1 data-number="2" id="background-establishing-rituals"><span
class="header-section-number">2</span> [Background] Establishing
rituals</h1>
<p>An ephemeral shift at the end of the 19th century saw advancements in
science and technology that provoked interest in mystical practices,
rituals and spirituality – sounds emerged from mysterious boxes, light
could appear inside a small glass bulb, and photography and cinema
pushed the boundaries of art and technology. Discourse in occultism
augmented as an antithetical practice to the emerging technologies of
the era.</p>
<p>Our contemporary relationship with mediating intelligence has led us
to once again explore esoteric practices to decipher wider meaning from
our developments in AI. People are turning to AI in a monotheistic way
in the same way that earlier generations turned to religion. Musings on
AI in the media are mostly prone to using the singular noun “an AI”-
resonating with Western monotheistic cultural practices.</p>
<p>The project attempts to establish a ritual, which aims to turn the
singular noun variant of “an AI” into its counterpoint- “artificial
intelligence”. An intelligence that is not centred in a small plastic
case, or a human or animal body, but instead as something animalistic,
distributed across many spirits and all encompassing.</p>
<h1 data-number="3" id="live-data-flows-and-time-travel"><span
class="header-section-number">3</span> Live data flows and time
travel</h1>
<p>Throughout our project we have jumped between imagining rituals and
creating technical systems, with it not always being clear which is
which. For example, as collaborating e-textile, live coding and
performance artists, working together required us to establish
meaningful dataflows and protocols for collaboration, which we could
also characterise as channels for carrying resonances between us.</p>
<p>The development and refinement of these custom technologies was
crucial in establishing the way we could interact. A core problem was
how to connect moving bodies with comparatively static actions of live
coders, typing at their laptops. With battery-powered e-Textile sensors
capturing movements and interactions with the textile surfaces, and
wireless networking acting as conduit, several problems around how to
interpret and respond to the data flows remained.</p>
<p>We focussed on our technological constraints as creative material,
and one such constraint was the delay or latency in our systems. There
were several steps in the processing and transmission of sensor data,
starting with the sensors themselves and the use of machine learning in
reducing the sensed dimensions. The most significant cause of latency
came from our use of the TidalCycles live coding system, however. By
default, for compatibility with low-powered hardware, Tidal adds a delay
of over 0.2s. This can be tuned to be much lower, but further
significant delay comes from the way that Tidal events are placed on
metrical cycles. In summary, both due to inherent delays and the way we
use Tidal to generate patterns, there could be a delay approaching half
a second before a live coder’s work could connect a sensed movement into
a sound triggered by it. Human perception is tuned to the speed of sound
<span class="citation" data-cites="chafe2010effect">(Chafe, Caceres, and
Gurevich 2010)</span>, so that any delay between seeing an action and
hearing the result is perceived as distance. Such a half-second delay
therefore translates to 171.5 metres, significantly dissociating the
movement and the result.</p>
<p>We approached this delay creatively, in three different ways. One
approach was to use the sensor in ways other than triggering sounds
(which can in any case be perceived as <em>too</em> direct, and heard as
‘mickey mouse’ style soundtracking). When applied to pattern
transformation or synthesis parameters, the movements shaped the music
in a less direct but still fundamental and tangible way. Another was to
reduce the latency for the interactions that did not involve live
coding. For example, sounds triggered from sensors placed on the faces
of the four performers were directly translated into sound with a custom
Python code according to simple ‘counting’ patterns. With different
performers triggering sounds at different rates, this counting created
polymetric interference patterns.</p>
<p>However for live coding interactions, we found a third, perhaps
counterintuitive approach to working with live sensor data. Rather than
minimising the delay, we instead explored increasing and compounding it.
This relied on ideas of patterned repetition and resonance that were
already a recurring theme in our discussions and collaborative work. For
example, where the piece worked within a metric cycle of 1.4 seconds,
and we found a latency of 0.4 seconds, to keep everything in time, we
simply had to delay the signal for an additional 1 second. On top of
this we added additional data ‘echoes’ of additional cycles, by applying
a cycle-length delay line to the data signal, with feedback. As a
result, repetitive movements fit perfectly to the metric cycle, and
built up and dissipated over time with the introduction and breaking of
the repetition.</p>
<h1 data-number="4" id="e-textile-sensors-and-magic-spells"><span
class="header-section-number">4</span> E-textile Sensors and magic
spells</h1>
<p>E-Textiles create soft and flexible sensors that are worn on the body
to sense movements. For this performance, we explored e-Textile pressure
sensors <span class="citation" data-cites="kobakant2009">(Perner-Wilson
2009-)</span> made of Eeontex stretch resistive fabric to sense bend and
stretch of garments caused by body movements. Embroidered capacitive
sensors were also used to detect touch or fold of fabric surfaces. In
early prototypes, we focused on pressure sensors placed proximate to the
body, for example embedded in tightly worn stretchy clothing, to detect
bend/stretch movements of the performers. We used a Bela mini board
which allowed us to read up to 8 analog sensors per performer and 1)
directly trigger sound from a single sensor data, 2) process multiple
sensor data with machine learning to be used as a reduced number of
synthesis parameters. We used ml.lib <span class="citation"
data-cites="ml-lib">(Momeni 2014)</span> externals on Pure Data <span
class="citation" data-cites="PureData">(Puckette 1996)</span> open
source software. E-Textile pressure sensors placed proximate to the
bodies capture precise movements well enabling the system to recognize
repetitions. However for the performers, these sensors create the notion
of “activation points”, which tend to become unwanted frameworks when
choreographing new movements.</p>
<p>In the later prototypes, we moved on to use conductive thread
embroidery on large textile surfaces (140cm x 280cm, 140cm x 140cm)
which function as capacitive touch sensors. We used them as 1) direct
trigger of a sound, 2) to create data patterns that influence live
coding patterns. When used in this way, the textile sensors capture
relative positions of the textile surface in relation with the
performer’s body.</p>
<p>When the data mapping is indirect with the sound outcome, we observed
that the onlookers are likely to misunderstand the correspondence and it
tends to give an impression that they are not interactive. Currently we
are evaluating above e-Textile sensor designs to decide for the final
design strategies, including how we use the data in the live coding
processes.</p>
<h2 data-number="4.1" id="machine-learning-and-e-textile-sensors"><span
class="header-section-number">4.1</span> Machine Learning and e-Textile
sensors</h2>
<p>In our first prototype, we used eight e-Textile pressure sensors on
performers’ bodies capturing every 40 milliseconds, streaming the data
simultaneously. These raw data are difficult to use as control
parameters, while one needs to monitor many sensors at once to make
sense of the incoming information. Reducing the dimensions of data using
machine learning became our practical solution. E-textile sensors often
show hysteresis and wear-and-tear as it is in continuous use and require
one to make fine tuning in process. Instead, assigning posture as
pattern of data and training the machine learning system reduces the
necessity of manual tuning. We observed this approach provides smoother
and more intuitive interaction between the wearer and the interactive
system</p>
<p>Fig x, xx, left: e-Textile pressure sensor embedded in undergarment,
right: e-Textile pressure sensor on socks measuring the foot pressure
against the floor. Both are from the early prototype costumes.
https://drive.google.com/file/d/16xm4f2iHlqv44PeveivKYNeVb3Kj1pIv/view?usp=share_link
https://drive.google.com/file/d/1frbX1SOChpX3GpkC6GohkZ1_RZil5ev_/view?usp=share_link</p>
<h2 data-number="4.2" id="materialising-ai-generated-images"><span
class="header-section-number">4.2</span> Materialising AI-Generated
Images</h2>
<p>We used Disco Diffusion <span class="citation"
data-cites="DiscoDiffusion">(Crowson 2021)</span> AI image generator to
produce a series of images with prompt texts composed of keywords taken
from the conceptual investigations, in order to create pattern design
for large scale digital print fabrics. Although a human designer created
a description of the image, reference style of drawings and an initial
image to influence compositions and the colour scheme, the main
“drawing” of the pattern is made by AI. Generally, AI-generated images
make mistakes in continuity of the space or geometry, which work against
the overall composition. Sometimes it generates strange associations of
image with prompt texts, that gives us a new, creative idea or
sensation. It is like a deformed mirror that reflects our
interpretations and associations of things. These AI image generation
tools can produce many variations of images in a relatively short time
and can inspire the creative process of a designer. But in the end, we
found this is not a ‘hands-off’ process. A human designer needs to
create their own interpretations and compositions for these images,
through hand-editing and collage, in order to complete it as an artistic
piece. In this sense one can say “artificial intelligence” is a powerful
creative support tool for us humans to make use of, but it is not
creative on its own.</p>
<p>Fig x xx, left: conductive thread embroidery as capacitive touch
sensor on a fabric, right: large size digital print fabric with AI
generated images
https://drive.google.com/file/d/1ILH97OerKcge9e-EkJ7bW8SAe1BEYLWV/view?usp=share_link
https://drive.google.com/file/d/16xm4f2iHlqv44PeveivKYNeVb3Kj1pIv/view?usp=share_link</p>
<h1 data-number="5" id="artificial-voices-and-warped-sounds"><span
class="header-section-number">5</span> Artificial Voices and Warped
Sounds</h1>
<p>TidalCycles was the main software used for live coding sounds <span
class="citation"
data-cites="mcleanMakingProgrammingLanguages2014">(McLean 2014)</span>,
however a few custom elements were employed to alter the territories
that live coders were used to, pushing them into new modes of
interacting with sound through software. The two live coders were tasked
with working independently when manipulating sensorial information from
the materials but also collaboratively in response to each other, to
create a unified sound world.</p>
<p>At the exposition of the piece, data from the sensors on the face of
the four performers was sent using the zero-mq (zmq) protocol <span
class="citation" data-cites="hintjens2013zeromq">(Hintjens 2013)</span>.
In this, the sensors’ were thresholded so that the movement of opening
of the mouth would lead to different sounds being triggered. A custom
python script would receive the trigger messages once above the
threshold, and play back a sample. The face sensor was then continuously
mapped to sample volume, so that the amplitude envelope of the sound
could be shaped or shortened with the mouth. Each performer was assigned
a unique set of sounds that would be triggered from their sensor. The
sounds that were triggered were made using an artificial voice, and the
outcome of this was to create a set of words that were nonsensical, but
through their repetition and cycling through the sound set would give
the impression of a “counting” sequence. This sequence was reduced from
six to two sounds during this part of the piece, creating tighter loops
and a greater sense of patterned interplay between the performers. To
create the sounds from an artificial voice, we used a Neural Audio
Synthesis model called RAVE <span class="citation"
data-cites="caillon2021rave">(Caillon and Esling 2021)</span>. In short,
this uses a variational autoencoder (VAE) structure but is applied as a
real-time model (able to generate 48kHz audio signals, while
simultaneously running 20 times faster than real-time on a standard
laptop CPU). The RAVE-model is trained on a corpus of audio and the VAE
structure decodes and then encodes the signals. The process of
“timbre-transfer” <span class="citation"
data-cites="caillon2021rave">(Caillon and Esling 2021)</span> allows
interpolation of the latent space based on input source sounds. These
input source sounds can be either audio files, or live input from a
microphone.</p>
<p>As well as being used to create a set of unique sounds to be
triggered by the opening movement of the mouth from the face sensors,
the RAVE model was also used for real-time vocal synthesis. Live vocal
input from a wireless microphone worn by one of the performers was
passed to this real-time model, which was implemented as an extension
for the PureData software. The real-time audio was fed into the model in
input and warped into an artificial voice by traversal through the
latent space. This allowed the manipulation of the human performer’s
voice and reconstructed it as an artificial and synthetic voice.</p>
<p>Finally, alongside the creation of artificial and vocable sounds, the
live coders also worked with the sensor data from the fabrics that were
manipulated through the performers’ movements to create music. The data
was also sent using the zmq protocol and through a custom script
received as osc-input data for TidalCycles. This allowed the live coders
to use the data to control aspects of the sound.</p>
<p>Various strategies were used in the translation of the data into
sound. One strategy that was employed by the live coders was to use the
touch of the textile could as a variable within the code, to control the
speed, pitch, gain at which sounds were played back or adjusting the
parameters of an audio effect applied to the pattern. This made the
relationship between sound and gesture apparent to the audience. When
adjusting some of the parameters, such as the perceptual loudness, it
was also useful to logarithmically scale the sensor inputs. An example
of this strategy shows how these features were implemented:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>plyWith <span class="dv">4</span> (<span class="op">#</span> speed <span class="st">&quot;&lt;0.5 2&gt;&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>   <span class="op">$</span> degradeBy <span class="st">&quot;&lt;0.25 0.36&gt;&quot;</span> <span class="op">$</span> sound <span class="st">&quot;&lt;residue fluid&gt;*8  # sus 0.4 # rel 0.4  # n (irand 64)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="st">   # djf (range 0.25 1 $ &quot;</span><span class="op">^</span>textile1b<span class="st">&quot;)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="st">   # gain (logBase 10 &lt;$&gt; range (1.6) (4) $ &quot;</span><span class="op">^</span>textile1a<span class="st">&quot;)</span></span></code></pre></div>
<p>Another strategy for performing with the sensors as the input source
was to use the data to trigger particular patterns depending on how much
interaction with the sensors embedded into the fabric occurred. This
involved creating a threshold over which the sound would be triggered.
This would allow the live coders to write various patterns and the
patterns would then be triggered through the shifting of the touch.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">do</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a> <span class="kw">let</span> textile i <span class="ot">=</span> (cR <span class="dv">0</span> (<span class="st">&quot;textile&quot;</span> <span class="op">++</span> <span class="fu">show</span> i))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a> <span class="kw">let</span> thresh <span class="ot">=</span> <span class="fl">0.7</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a> d1</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>   <span class="op">$</span> sometimes (plyWith <span class="dv">4</span> (<span class="op">#</span> speed <span class="st">&quot;0.25&quot;</span>))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>   <span class="op">$</span> stack [</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>     mask ((<span class="op">&gt;</span> thresh) <span class="op">&lt;$&gt;</span> textile <span class="dv">0</span>) <span class="op">$</span> sound <span class="st">&quot;~ &lt;sfe:4*2 sfe:8&gt; [~ glitch] ~&quot;</span>, </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>     mask (( <span class="op">&gt;</span>thresh) <span class="op">&lt;$&gt;</span> textile <span class="dv">1</span>) <span class="op">$</span> gain <span class="st">&quot;0 1 0 [0 1] [0 1] 0 1 1&quot;</span> <span class="op">#</span> sound <span class="st">&quot;cckit1&quot;</span>   <span class="op">#</span> n (irand <span class="dv">8</span>),    </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>     mask (( <span class="op">&gt;</span>thresh) <span class="op">&lt;$&gt;</span> textile <span class="dv">2</span>) <span class="op">$</span> sound <span class="st">&quot;[~ hh] [~ hh]? [~ hh] [~ hh]?&quot;</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>     mask (( <span class="op">&gt;</span>thresh) <span class="op">&lt;$&gt;</span> textile <span class="dv">3</span>) <span class="op">$</span> sound <span class="st">&quot;cubic:7/2&quot;</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>   ]</span></code></pre></div>
<p>Through these strategies we found that the live coders’ fingers were
relieved from the burden of typing where the performers bodies became a
new input source to update and shape the code in real-time.</p>
<p>Overall, the top down view of how the piece came together in
performance and how the different sounds combined can be seen in the
figure below.</p>
<figure>
<img src="images/flow-figure1.png"
alt="Flow of how all the pieces for the “Patterns in between Intelligences” performance materialised" />
<figcaption aria-hidden="true">Flow of how all the pieces for the
“Patterns in between Intelligences” performance
materialised</figcaption>
</figure>
<h1 data-number="6" id="staging-ai"><span
class="header-section-number">6</span> Staging AI</h1>
<p>As we started off with responding to the call for AI in creative
practices, we looked into how we can incorporate AI technology in our
collaborative works. We often discussed whether we treat AI as a tool or
whether it plays a role as a subject matter.</p>
<p>Here are some of the discussions we explored:</p>
<ul>
<li>AI is a “difficult” subject matter - AI is widely discussed and
criticised in our society, and we have certain expectations of AI
technology, how it looks and what it does, such as what we see in
science fiction. This makes it complex to stage AI if you do not want to
“fake” the technology, especially in a non-text based (i.e. not
narrative theatre) performance piece.</li>
<li>AI tools accessible for creative coders are limited and do not live
up to what we know from popular science. On the other hand, it is not
our intention to make the piece as a cutting-edge technology demo. We do
not want to be an artistic ‘sweetener’ for the newest AI technology
showcase, but how do we meet the expectation of what we imagine as
AI?</li>
<li>Fighting with expectation of “stage magic” - For spectators,
everything that happens on a stage is a spectacle. How do we show, or
not show the technology behind our work, especially when technology is a
subject matter? Our response to this is framed by our artistic team
including proponents of live coding, open source and open hardware.
Moreover, how can we be open about our work when AI is so associated
with mysterious ‘black box’ technology?</li>
<li>Distinction between live composition and audience attention and
engagement - How to keep the action fresh and spontaneous while at the
same time keeping the attention of the action focused? In a way AI
technologies have more agency to make decisions live on stage but these
decisions are barely noticeable unless you fall back to explaining
them.</li>
</ul>
<h1 data-number="7"
id="what-is-artificial-intelligence-to-pattern-makers"><span
class="header-section-number">7</span> What is “artificial intelligence”
to pattern-makers?</h1>
<p>To begin, a discussion of what “artificial intelligence” entails is
offered. A generic definition is offered in the Oxford English
Dictionary:</p>
<blockquote>
<p>“Artificial intelligence: the theory and development of computer
systems able to perform tasks that normally require human intelligence,
such as visual perception, speech recognition, decision-making, and
translation between languages” <span class="citation"
data-cites="oxfordreference">(<span>“Artificial Intelligence,”</span>
n.d.)</span>.</p>
</blockquote>
<p>Alan Turing also famously considered the question “Can machines
think?” in his seminal work Computing Machinery and Intelligence <span
class="citation" data-cites="turing2009computing">(Turing 2009)</span>.
He asserts that, stated this way, the question is “too meaningless to
deserve discussion”. Moreover, the status of “intelligence” in Turing’s
paper is never made specific and apart from in the title, the term only
appears once, clearly referring to human intelligence, and the reader is
generally invited to infer the relationship between “can…think” and “is
intelligent.” Many definitions of Artificial Intelligence are similarly
embedded in terms of “things a human can or might do” (see <span
class="citation" data-cites="bellman1978introduction">(Bellman
1978)</span> <span class="citation"
data-cites="haugeland1985artificial">(Haugeland 1985)</span>). From
these definitions, what this means for live coders, e-textile artists
and performers is considered, where each of these mediums deal with the
expression of patterns. We ask- is a machine learning algorithm
intelligent when intelligence is discussed in these terms? How about a
choreographic system, or an electronic circuit woven into textile, do
they qualify? Or perhaps do they still exceed the intelligence of a
machine learning algorithm?</p>
<p>The word ‘intelligence’ is perhaps even more problematic, when the
history of measurements of intelligence (IQ tests) is wrapped in the
racist ideology of the eugenics movement, what can we consider as
“knowledge and skills” and what it means to “acquire and apply”?.
Intelligence generally implies a separation from the mind and body as
the definition in the dictionary already illustrates: as if the skills
of bodily action are outside of the knowledge acquired in mind. While we
should be careful in over-romanticising the history of handcrafts, we
nonetheless argue that situating intelligence in craft practices demands
a less dualistic approach, rather seeing intelligence of mind as
inseparable from the intelligence of the body and indeed the collective
intelligence of a community of practice.</p>
<p>While we incorporate contemporary tools in our systems that overtly
describe themselves as AI and ML (machine learning) technologies, like
dimensional reduction algorithms, pattern recognition systems and neural
audio synthesis, one of our background interests in AI is in its
relation to the ‘heritage algorithms’ of traditional pattern-making, in
music and textile. In essence, many traditional approaches to
pattern-making are a form of computational creativity. For example in
weaving, the tie-ups of shaft looms combine binary sequences through
matrix multiplication to create complex, three dimensional structures
from simple treddling patterns. This creates very rich creative ground,
supporting a traditional and continually evolving practice that still
finds new structural techniques despite the technological development
over millennia. This human-driven, computational pattern generation is a
clear analogue to pattern recognition techniques in AI. A longer term
challenge for our collaboration that brings together dance, e-Textile
and code artists/technologists is to find ways to develop new technology
that is respectful and (to some extent) literally interwoven with such
heritage technologies, to create a properly grounded approach to AI.</p>
<h1 data-number="8"
id="sheffield-prototype-patterns-in-between-intelligences"><span
class="header-section-number">8</span> Sheffield prototype: Patterns In
Between Intelligences</h1>
<p>The first (and at the time of writing, latest) iteration of our live
performance debuted at No Bounds Festival in Sheffield UK, as a
prototype of the overall project. This performance provided the test bed
to implement ideas in front of a live audience, and see how they shape
the performance as a real-time feedback channel. A narrative account of
the performance follows.</p>
<p>https://drive.google.com/file/d/1wL3cPEFlNi9IBxypkyLWHKi7kb9UDbPd/view</p>
<figure>
<img src="images/sheffield1.jpg"
alt="Performance of “Patterns in between intelligences” at No Bounds Festival, Sheffield" />
<figcaption aria-hidden="true">Performance of “Patterns in between
intelligences” at No Bounds Festival, Sheffield</figcaption>
</figure>
<blockquote>
<p>This performance can be considered a site specific piece, as we
brought together these elements but staged them specifically to achieve
a close interaction with an audience that was present in a big church.
Staging refers to the composition of the different elements and
disciplines to achieve a whole thread or narrative based on the
experience of the audience. In this case choreography with body
movements and gestures, sounds and text produced by the human voice, and
the overall hints at a stage situation were the main staging of the
bodies and the objects.</p>
</blockquote>
<blockquote>
<p>At the beginning the audience gathers in the church after purchasing
their tickets and some drinks, the initial expectation of this audience
relates to the specificity of this event and the location (an electronic
music festival and a cathedral). The space was lit with cold light, but
given the dimensions of the cathedral the background and surroundings of
the center was very dark, hinting at the magnitude of the building.
There was no sound, just the chatter of the audience was hearable. In
the brightest part of the space, the area inside the columns, we see a
textile hanging from a tripod, and some folded fabrics of a color
similar to the floor. In the edges of this space and behind the area
inside the columns we see other tripods, a desk with the sound and light
technicians, a stage, as well as all the symbology and paraphernalia of
a church.</p>
</blockquote>
<blockquote>
<p>At some point a woman dressed in black with a sensor on her face and
a headset microphone stands next to a group of audience members, gets
close to them as if she was one of the visitors of the festival, she
opens her mouth, triggering a loud voice sound that interrupts the
silence, echoing throughout the cathedral. This sound disruption made
the overall volume of the audience chatter lower for a moment, most of
them were not aware of where it came from, it might just have been a
sound checking error. The woman moves slowly within the group of
audience members, some of them notice her presence. She suddenly opens
her mouth again a couple more times, triggering the loud sound again.
The overall volume of the chatter lowers and at some point the audience
knows that something started, changing the mood in the space. At this
point though, not everyone knows what is going on, as the woman is very
much one of the many bodies present in space.</p>
</blockquote>
<blockquote>
<p>She then goes next to a column and most people get to see how when
she opens her mouth a sound is produced in the whole space. At some
point other bodies, also wearing black clothes and face sensors join the
space, some of them are next to the columns others in the background. At
this point the sounds being produced happen more often and the relation
between the gesture of some people wearing black clothes and sensors on
their face becomes clear. The sounds voice recordings as if they were
snippets of someone talking, murmurs, gibberish, and they repeat
independently of who opens their mouth. The people producing the sounds
gather next to the column of the space and at the distance they exchange
looks, as if they were playing with the timing of when to open the
mouths and produce sound, but also as if they were communicating in some
way. They change positions and create different constellations,
sometimes two of them come closer, sometimes they observe from a
distance. The accumulation of the voice sounds becomes more and more
present, but their gestures are simple and it contrasts with how they
move around and space. It is a very mechanical and minimal sudden mouth
movement.</p>
</blockquote>
<blockquote>
<p>Then they gather next to the hanging fabric and the sounds and mouth
movements become faster, as if you see a group of four people discussing
something, the sounds they produce, these voice snippets overlap and
accumulate filling the whole cathedral. Two of them then sit on the
floor next to the laptops visible in the space and start typing
something. The other ones do some more of the mouth opening gestures but
now the way they walk and their interactions involve going towards each
other and creating a spinning trajectory, walking and facing each other
while they produce the sound. They smile and share a complicity between
them as well as with the ones sitting on the computer as if they were
plotting something in that space.</p>
</blockquote>
<blockquote>
<p>At this moment somewhere far from the dark a buzzing sound is heard
and from behind some of the audience out of the darkness we see a person
wearing some kind of robe and holding a string attached to a small
drone. The drone flies over the audience into the space and behind the
columns and the person holding it looks very concentrated at it,
sometimes the drones pull this person and they follow, other times it
looks like the drone comes to them, you can’t be completely sure who is
controlling who. The person enters the area inside the columns and the
drone goes to one of the folded fabrics on the floor, the wind it
produces unfolds part of it and the attention of the other performers
seem to be directed at that moment. The initial surprise of this figure
that entered the space with a drone, wearing an outfit that is not black
clothes and the fact that while the drone flies over the cloth,
transforms this surprise into an intrigue and curiosity of what are
these cloths that have been laying down all this time and a curiosity of
what other figures might appear from the darkness of the cathedral. The
figure with the drone leaves in the opposite direction, she takes off
the robe and disappears behind the space into the darkness.</p>
</blockquote>
<blockquote>
<p>A subtle sound is heard and still some of the voices from the
beginning. At this point the woman that first appeared has all her
attention directed towards the fabric on the floor and slowly walks
towards it. She bends down and starts to unfold it, extending it on the
floor. We see an image, similar to the one hanging on the tripod but in
a different color. She looks at it and then kneels down to touch it and
when she touches it a distorted and loud sound is heard. She caresses
the fabric and the sound changes in accordance to how she moves her hand
over the surface of the fabric. The sound also stops when she takes her
hand away. She looks in different directions, reacting to something that
happens every time she touches the fabric, and she whispers some text as
if she was talking to someone in the darkness or as if every time she
touches the fabric it makes her speak. We just cannot understand what
she’s saying because it’s fast, it’s being whispered, and the sounds
(the background sound, the sound produced by touching the fabric, and
the occasional voice sounds of the sensors grow louder and louder). The
intensity of this interaction grows together with the movement of the
person, the intensity of her whispers, and the intensity of the
sound.</p>
</blockquote>
<blockquote>
<p>While this is happening in the background one of the four bodies with
the face sensors looks at this action and sees a fabric folded next to
him. He approaches it and unfolds it, maybe inspired by what the other
person is doing or maybe out of curiosity. We get a glimpse of another
image in the fabric but this one is never fully extended on the floor.
He then proceeds to wrap himself with it, wearing similar to the figure
with the drone but never fully wearing it properly. He seems to be
struggling with it, he seems to be feeling its texture, he seems to be
stretching it around himself. His movements become more erratic and it
looks like by wearing the fabric the way he moves got transformed by it.
He is more in the background, in the space behind the columns, close to
the audience. His gaze is directed towards the horizon and even though
he gets very close to some audience members and in the outbursts of his
sudden movements almost hit them, he is not really present but
addressing something else in the space. He also moves his mouth as if he
was mumbling something but he doesn’t whisper like the other person. His
movements grow more intense and erratic, as if being controlled by the
sound, he goes inside the area of the columns.</p>
</blockquote>
<blockquote>
<p>We see both bodies grow in intensity, the woman just opens another
fabric and the whispering transforms into voice and body glitches. The
sound intensity keeps on growing, it accentuates the speed and scope of
the movements of the bodies (which also gradually grew) but also you are
not sure if they are following the music or if it’s their bodies and the
fabrics controlling the intensity. This moment grows and it becomes
darker, from the repetition of these sounds and the movement vocabulary
being repeated, the musical buildup becomes an intense techno moment.
The two bodies that were interacting with the fabrics start extending
all the other fabrics on the floor, revealing the images on them. Two
other persons, one of them the drone figure, hang the remaining fabric
on the tripod outside of the area with columns. The situation of how the
bodies interacted with the fabrics, the whispers and the movements, as
well as the kind of music they were building together transformed into a
regular and repetitive beat and the space transformed into a display of
a series of huge images, each one in a different color scheme and
containing different figures, maybe related to the bodies we have
seen.</p>
</blockquote>
<blockquote>
<p>The two performers that extended the fabrics and have been
interacting with them start doing a repetitive movement on top of them,
spinning their torsos and arms and heads in synchronisation with the
beat. Once the audience sees all the images and also supported by a
stroboscopic light they seem to have entered a trance, enhanced by the
techno as in a rave. At one point they collapse against the wall and
move in slow motion against it. The audience gets the chance to
appreciate the whole composition that all the fabrics produce together
and the space becomes a club or rave space. One of the bodies gets
stands, the loud music suddenly stops, and he approaches the audience
and addresses them. He talks directly to the audience, very fast nd in
close proximity.</p>
</blockquote>
<blockquote>
<p>He says the following text: “I see mystical patterns amongst the
noise. What is behind it all? I see five archetypes. I see the new age
possessed jester of ai. I see the machine learning weaving of ancestral
knowledge. I see the mother performing neural endurance rituals. I see
the synthetic choir queen of information technologies. I see the pagan
data center workers ritualizing spiritual cables. I see these five
archetypes, and there is something behind them. It’s like a bill. The
worth is in the image printed on it and not in the paper itself. There
is an intelligence behind all this! There is, there is! But who’s
responsible for it? It’s not about the content, it is about the
context.” You nevertheless cannot fully understand the content of his
speech because it is very fast, he interrupts the amplification of it by
opening his mouth, triggering the sound of the sensors, and he repeats
some of the sentences, strangening the way the audience perceives the
content. It looks like an attempt to explain what has just happened but
distorted in such a way that it becomes more like a repetitive sound
similar to the techno music that was just heard.</p>
</blockquote>
<blockquote>
<p>The other body who is still laying on the floor starts to accompany
this speech cacophony by contrasting it with a repetitive and slow
singing in a high pitch. The voice seems to be distorted, we then
realise that everytime she sings other voices appear through the
amplified sound, since no one else is singing. The man talking leaves,
continuing to talk but fading out in the darkness of the cathedral. The
person singing also approaches the audience and sings close to them, we
realise, in the proximity that there is a relation between her voice and
the amplified voice intervals from the amplification. At one point she
also leaves into the darkness. Her voice is still heard in the space in
repetitive tones. After she leaves all the other music is also gone and
her voice stays like a mantra. The people sitting on the laptops stand
up and leave. We are only left with the voice and the emptiness of the
room. After some cycles and repetitions we appreciate the emptiness of
the room and the fabrics hanging or laying on the floor, we see other
audience members staring into the space, and due to its length we
revisit what just happened and how the initial space and situation
transformed. At some point the final repetition of the mantra happens
and the lights go out, leaving the audience in complete darkness.</p>
</blockquote>
<h1 data-number="9" id="reflections"><span
class="header-section-number">9</span> Reflections</h1>
<p>The performance of “Patterns In Between Intelligences” was an
exploration of new avenues of collaboration between the media of live
coding, e-textiles and movement. Bodies manipulated fabrics, which in
turn wirelessly fed the code with data to affect its outcome. However,
in our performance we did not just hope to create these connections with
each other. Beyond this, the piece also functions to achieve some wider
aims, that looks to both critique contemporary uses of AI and also
uphold a paradigm for a shared future with it.</p>
<p>In particular, throughout the piece we wanted to reframe AI as both a
practice rooted in tradition and esotericism, rather than a rationalist
one from ‘cutting edge’ technological practices. AI is rapidly being
expanded as the newest, and sometimes the sole focus, wing of the field
technology. However, the etymology of the word “technology” holds its
roots as the “systematic treatment of an art, craft or technique” <span
class="citation" data-cites="etymology">(<span>“Technology (n.),”</span>
n.d.)</span>, where this in turn stems from the Greek word “tekhnē”
meaning “art” or “craft”. Technology and craft practices are never
ontologically as far from each other as we might assume.</p>
<p>Moreover, this performance also explored how AI functions as a mirror
for contemporary societal issues. If most current AI techniques are fed
using corpuses of contemporary dataset (e.g.images, music) scraped from
the internet, then perhaps we can think of it as a mapping of our
contemporary and somewhat broken society. The internet can be treated as
a metaphor or manifestation of what Jung refers to as the “collective
unconscious” <span class="citation" data-cites="jung2014collected">(Jung
2014)</span>. Common archetypes have sometimes begun to emerge through
uses of artificial intelligence algorithms (such as the Disco Diffusion
generation algorithms mentioned). These archetypes can veer towards
caricatures- strong visual figures like Donald Trump, Mario, or Ronald
McDonald are vivid and clear for everyone, but abstract and esoteric
concepts like “gnosis” or “democracy” are less so. To open new vistas to
alternative paradigms of society, the datasets and training should be
emblematic of the world we wish to create, not the one we wish to grow
beyond.</p>
<h1 data-number="10" id="acknowledgements"><span
class="header-section-number">10</span> Acknowledgements</h1>
<p>This work was supported by the Volkswagen Foundation LINK masters
Grant and the Fonds Darstellende Künste Foundation Grant. Additionally,
Wilson’s contributions were supported by EPSRC and AHRC under the
EP/L01632X/1 (Centre for Doctoral Training in Media and Arts Technology)
grant and Alex McLean’s contributions were supported by a UKRI Future
Leaders Fellowship [grant number MR/V025260/1].</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-oxfordreference" class="csl-entry" role="doc-biblioentry">
<span>“Artificial Intelligence.”</span> n.d. <em>Oxford Reference</em>.
<a
href="https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095426960;jsessionid=F88D42EEB3615E6009C44E40C556515F">https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095426960;jsessionid=F88D42EEB3615E6009C44E40C556515F</a>.
</div>
<div id="ref-bellman1978introduction" class="csl-entry"
role="doc-biblioentry">
Bellman, Richard. 1978. <em>An Introduction to Artificial Intelligence:
Can Computers Think?</em> Thomson Course Technology.
</div>
<div id="ref-caillon2021rave" class="csl-entry" role="doc-biblioentry">
Caillon, Antoine, and Philippe Esling. 2021. <span>“RAVE: A Variational
Autoencoder for Fast and High-Quality Neural Audio Synthesis.”</span>
<em>arXiv Preprint arXiv:2111.05011</em>.
</div>
<div id="ref-chafe2010effect" class="csl-entry" role="doc-biblioentry">
Chafe, Chris, Juan-Pablo Caceres, and Michael Gurevich. 2010.
<span>“Effect of Temporal Separation on Synchronization in Rhythmic
Performance.”</span> <em>Perception</em> 39 (7): 982–92.
</div>
<div id="ref-DiscoDiffusion" class="csl-entry" role="doc-biblioentry">
Crowson, Russell, Katherine. 2021. <span>“Disco Diffusion.”</span>
</div>
<div id="ref-haugeland1985artificial" class="csl-entry"
role="doc-biblioentry">
Haugeland, John. 1985. <span>“Artificial Intelligence: The Very
Idea.”</span> Cambridge, MA: MIT Press.
</div>
<div id="ref-hintjens2013zeromq" class="csl-entry"
role="doc-biblioentry">
Hintjens, Pieter. 2013. <em>ZeroMQ: Messaging for Many
Applications</em>. " O’Reilly Media, Inc.".
</div>
<div id="ref-jung2014collected" class="csl-entry"
role="doc-biblioentry">
Jung, Carl Gustav. 2014. <em>Collected Works of CG Jung, Volume 9 (Part
1): Archetypes and the Collective Unconscious</em>. Vol. 48. Princeton
University Press.
</div>
<div id="ref-mcleanMakingProgrammingLanguages2014" class="csl-entry"
role="doc-biblioentry">
McLean, Alex. 2014. <span>“Making Programming Languages to Dance to:
<span>Live Coding</span> with <span>Tidal</span>.”</span> In
<em>Proceedings of the 2nd <span>ACM SIGPLAN International
Workshop</span> on <span>Functional Art</span>, <span>Music</span>,
<span>Modelling</span> and <span>Design</span></em>.
<span>Gothenburg</span>. <a
href="https://doi.org/10.5281/zenodo.7452894">https://doi.org/10.5281/zenodo.7452894</a>.
</div>
<div id="ref-ml-lib" class="csl-entry" role="doc-biblioentry">
Momeni, Bullock, Ali. 2014. <span>“Ml-Lib.”</span>
</div>
<div id="ref-kobakant2009" class="csl-entry" role="doc-biblioentry">
Perner-Wilson, Satomi, Hannah. 2009-. <span>“Bonded Bend Sensor,
Kinesiology Tape Bend Sensor.”</span>
</div>
<div id="ref-PureData" class="csl-entry" role="doc-biblioentry">
Puckette, Miller. 1996. <span>“Pure Data.”</span>
</div>
<div id="ref-etymology" class="csl-entry" role="doc-biblioentry">
<span>“Technology (n.).”</span> n.d. <em>Etymology</em>. <a
href="https://www.etymonline.com/word/technology">https://www.etymonline.com/word/technology</a>.
</div>
<div id="ref-turing2009computing" class="csl-entry"
role="doc-biblioentry">
Turing, Alan M. 2009. <span>“Computing Machinery and
Intelligence.”</span> In <em>Parsing the Turing Test</em>, 23–65.
Springer.
</div>
</div>
</body>
</html>
